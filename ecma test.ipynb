{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ceedff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True effect of schooling on wages: 3\n",
      "True effect of treatment on wages: 1.5\n",
      "Estimated effect of schooling (2SLS): 3.0342\n",
      "Estimated effect of treatment (2SLS): 1.3120\n",
      "Estimated ATE: 7.2155\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Set seed for reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "# Simulate data\n",
    "N = 1000  # Number of observations\n",
    "\n",
    "# Instrumental variable (e.g., distance to college)\n",
    "Z = np.random.normal(0, 1, N)\n",
    "\n",
    "# Treatment variable (e.g., receiving a scholarship)\n",
    "T = np.random.binomial(1, 0.5, N)  # 50% chance of treatment\n",
    "\n",
    "# Error terms\n",
    "epsilon = np.random.normal(0, 1, N)  # Wage shock\n",
    "nu = np.random.normal(0, 1, N)  # Schooling shock\n",
    "\n",
    "# Generate schooling variable (affected by treatment and instrument)\n",
    "S = 2 + 1.5 * Z + 2.0 * T + nu  # Treatment increases schooling\n",
    "\n",
    "# True wage equation (schooling and treatment affect wages)\n",
    "wages = 10 + 3 * S + 1.5 * T + epsilon  # True causal effects\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Wages': wages, 'Schooling': S, 'Instrument': Z, 'Treatment': T})\n",
    "\n",
    "# ----- First-stage regression: Schooling on Instrument and Treatment -----\n",
    "X_first_stage = np.column_stack((Z, T))  # IV and Treatment as predictors\n",
    "first_stage_model = LinearRegression()\n",
    "first_stage_model.fit(X_first_stage, S)\n",
    "S_hat = first_stage_model.predict(X_first_stage)  # Predicted schooling\n",
    "\n",
    "# ----- Second-stage regression: Wages on predicted schooling and treatment -----\n",
    "X_second_stage = np.column_stack((S_hat, T))  # Predicted schooling + Treatment\n",
    "second_stage_model = LinearRegression()\n",
    "second_stage_model.fit(X_second_stage, wages)\n",
    "beta_s = second_stage_model.coef_[0]  # Effect of schooling on wages\n",
    "beta_t = second_stage_model.coef_[1]  # Effect of treatment on wages\n",
    "\n",
    "# ----- Compute ATE -----\n",
    "E_S_T1 = np.mean(S[T == 1])  # Expected schooling for treated group\n",
    "E_S_T0 = np.mean(S[T == 0])  # Expected schooling for control group\n",
    "\n",
    "ATE = beta_t + beta_s * (E_S_T1 - E_S_T0)\n",
    "\n",
    "# Print results\n",
    "print(f\"True effect of schooling on wages: 3\")\n",
    "print(f\"True effect of treatment on wages: 1.5\")\n",
    "print(f\"Estimated effect of schooling (2SLS): {beta_s:.4f}\")\n",
    "print(f\"Estimated effect of treatment (2SLS): {beta_t:.4f}\")\n",
    "print(f\"Estimated ATE: {ATE:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac40344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Expected Schooling for Treated (T=1): 3.9771\n",
      "True Expected Schooling for Control (T=0): 2.0315\n",
      "True ATE: 7.3369\n"
     ]
    }
   ],
   "source": [
    "# True coefficients\n",
    "beta_s_true = 3.0  # True effect of schooling on wages\n",
    "beta_t_true = 1.5  # True direct effect of treatment on wages\n",
    "\n",
    "# Compute expected schooling for treated and control groups\n",
    "E_S_T1_true = np.mean(S[T == 1])  # Mean schooling for treated group\n",
    "E_S_T0_true = np.mean(S[T == 0])  # Mean schooling for control group\n",
    "\n",
    "# Compute True ATE\n",
    "true_ATE = beta_t_true + beta_s_true * (E_S_T1_true - E_S_T0_true)\n",
    "\n",
    "# Print results\n",
    "print(f\"True Expected Schooling for Treated (T=1): {E_S_T1_true:.4f}\")\n",
    "print(f\"True Expected Schooling for Control (T=0): {E_S_T0_true:.4f}\")\n",
    "print(f\"True ATE: {true_ATE:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20371cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'generated_data.csv'\n",
      "Estimated effect of schooling (2SLS): 3.0357\n",
      "Estimated effect of treatment (2SLS): 1.5721\n",
      "True ATE: 7.9108\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate data\n",
    "N = 1000  # Number of observations\n",
    "\n",
    "# Instrumental variable (e.g., distance to college)\n",
    "z1 = np.random.normal(0, 1, N)\n",
    "\n",
    "# Treatment variable (e.g., receiving a scholarship)\n",
    "t1 = np.random.binomial(1, 0.5, N)  # 50% chance of treatment\n",
    "\n",
    "# Error terms\n",
    "epsilon = np.random.normal(0, 1, N)  # Wage shock\n",
    "nu = np.random.normal(0, 1, N)  # Schooling shock\n",
    "\n",
    "# Generate schooling variable (affected by treatment and instrument)\n",
    "x1 = 2 + 1.5 * z1 + 2.0 * t1 + nu  # Treatment increases schooling\n",
    "\n",
    "# True wage equation (schooling and treatment affect wages)\n",
    "y1 = 10 + 3 * x1 + 1.5 * t1 + epsilon  # True causal effects\n",
    "\n",
    "# Create DataFrame with labeled columns\n",
    "df = pd.DataFrame({\n",
    "    'y1': y1,        # Outcome: Wages\n",
    "    'x1': x1,        # Other variable: Schooling\n",
    "    'z1': z1,        # Instrument\n",
    "    't1': t1         # Treatment\n",
    "})\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('generated_data.csv', index=False)\n",
    "print(\"Data saved to 'generated_data.csv'\")\n",
    "\n",
    "# ------------------------- 2SLS Implementation -------------------------\n",
    "# Stage 1: Regress treatment (t1) on instrument (z1)\n",
    "X_first_stage = np.column_stack((z1, t1))  # IV and Treatment as predictors\n",
    "first_stage_model = LinearRegression()\n",
    "first_stage_model.fit(X_first_stage, x1)\n",
    "x1_hat = first_stage_model.predict(X_first_stage)  # Predicted schooling\n",
    "\n",
    "# Stage 2: Regress outcome (y1) on predicted schooling (x1_hat) and treatment (t1)\n",
    "X_second_stage = np.column_stack((x1_hat, t1))  # Predicted schooling + Treatment\n",
    "second_stage_model = LinearRegression()\n",
    "second_stage_model.fit(X_second_stage, y1)\n",
    "beta_s_2sls = second_stage_model.coef_[0]  # Effect of schooling on wages (from 2SLS)\n",
    "beta_t_2sls = second_stage_model.coef_[1]  # Effect of treatment on wages (from 2SLS)\n",
    "\n",
    "# Print 2SLS estimates\n",
    "print(f\"Estimated effect of schooling (2SLS): {beta_s_2sls:.4f}\")\n",
    "print(f\"Estimated effect of treatment (2SLS): {beta_t_2sls:.4f}\")\n",
    "\n",
    "# ------------------------- ATE Computation -------------------------\n",
    "# Compute expected schooling for treated and control groups\n",
    "E_S_T1 = np.mean(x1[t1 == 1])  # Mean schooling for treated group\n",
    "E_S_T0 = np.mean(x1[t1 == 0])  # Mean schooling for control group\n",
    "\n",
    "# Compute True ATE using the formula\n",
    "beta_s_true = 3.0  # True effect of schooling on wages\n",
    "beta_t_true = 1.5  # True direct effect of treatment on wages\n",
    "\n",
    "true_ATE = beta_t_true + beta_s_true * (E_S_T1 - E_S_T0)\n",
    "\n",
    "# Print results\n",
    "print(f\"True ATE: {true_ATE:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "504fe757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"generated_data.csv\")\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "val_df.to_csv(\"valid.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f30c223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mliv.inference import Vanilla2SLS\n",
    "from mliv.utils import CausalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "523b5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run -1-th experiment for Vanilla2SLS. \n",
      "End. --------------------\n"
     ]
    }
   ],
   "source": [
    "data = CausalDataset('./')\n",
    "\n",
    "model = Vanilla2SLS()\n",
    "model.fit(data)\n",
    "ITE = model.predict(data.train)\n",
    "ATE,_ = model.ATE(data.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b136d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4505586695037187\n"
     ]
    }
   ],
   "source": [
    "print(ATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4f139d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (ATE): 1.555065784936636\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"generated_data.csv\")\n",
    "\n",
    "# First Stage: Regress the treatment (t1) on the instrument (z1) and other regressor (x1)\n",
    "X_first_stage = sm.add_constant(data[['z1', 'x1']])  # Add constant for intercept\n",
    "first_stage = sm.OLS(data['t1'], X_first_stage).fit()\n",
    "\n",
    "# Get the fitted values from the first stage (predicted treatment)\n",
    "fitted_treatment = first_stage.fittedvalues\n",
    "\n",
    "# Second Stage: Regress the outcome (y1) on the fitted treatment and other regressor (x1)\n",
    "X_second_stage = sm.add_constant(data[['x1']])  # Add constant for intercept\n",
    "second_stage = sm.OLS(data['y1'], X_second_stage.assign(t1=fitted_treatment)).fit()\n",
    "\n",
    "# The coefficient on the fitted treatment is the ATE\n",
    "ate = second_stage.params['t1']\n",
    "print(f\"Average Treatment Effect (ATE): {ate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf067f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_stata(\"angrist.dta\")\n",
    "df = df.rename(columns={col: col.replace('v', 'x') for col in df.columns})\n",
    "df = df.rename(columns={'x18': 'z1', 'x4': 't1', 'x9': 'y1'})\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f45c90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   T  Z        X1        X2        X3         Y\n",
      "0  1  0 -1.406317  0.874517 -0.150320  3.772121\n",
      "1  1  1 -0.083106 -0.649765 -0.326696  2.235624\n",
      "2  1  1 -1.504720 -1.203201 -1.042578 -1.559216\n",
      "3  1  1  0.760056 -1.042044 -1.172234  1.869701\n",
      "4  1  0  0.082440 -0.487203  0.464370  4.398882\n",
      "True ATE: 2.9258682265361906\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples\n",
    "n = 1000\n",
    "\n",
    "# Generate instrumental variable (Z), assume it's binary\n",
    "Z = np.random.binomial(1, 0.5, size=n)\n",
    "\n",
    "# Generate treatment variable (T), assume treatment is assigned based on the instrument (Z) and some noise\n",
    "T = (0.5 * Z + np.random.normal(0, 0.2, size=n) > 0).astype(int)\n",
    "\n",
    "# Generate some other regressors (X1, X2, X3)\n",
    "X1 = np.random.normal(0, 1, size=n)\n",
    "X2 = np.random.normal(0, 1, size=n)\n",
    "X3 = np.random.normal(0, 1, size=n)\n",
    "\n",
    "# Assume the outcome variable (Y) is a linear function of the treatment, instrument, and other covariates\n",
    "# Y = Beta_T * T + Beta_X1 * X1 + Beta_X2 * X2 + Beta_X3 * X3 + noise\n",
    "Beta_T = 3  # Treatment effect coefficient\n",
    "Beta_X1 = 1  # Regressor 1 effect coefficient\n",
    "Beta_X2 = 2  # Regressor 2 effect coefficient\n",
    "Beta_X3 = -1  # Regressor 3 effect coefficient\n",
    "\n",
    "# Generate outcome variable Y\n",
    "Y = Beta_T * T + Beta_X1 * X1 + Beta_X2 * X2 + Beta_X3 * X3 + np.random.normal(0, 1, size=n)\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'T': T,\n",
    "    'Z': Z,\n",
    "    'X1': X1,\n",
    "    'X2': X2,\n",
    "    'X3': X3,\n",
    "    'Y': Y\n",
    "})\n",
    "\n",
    "# Print the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Calculate the true Average Treatment Effect (ATE)\n",
    "# True ATE is the difference in the mean outcome for treated and untreated individuals\n",
    "ate_true = np.mean(Y[T == 1]) - np.mean(Y[T == 0])\n",
    "print(f\"True ATE: {ate_true}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6559e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = df[['y1', 't1', 'z1', \"x1\", \"x10\", \"x19\"]]\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "val_df.to_csv(\"valid.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9eacf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['y1', 't1', 'z1', 'x1', 'x10', 'x19'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe42c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:                     y1   R-squared:                      0.9680\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                 0.9680\n",
      "No. Observations:              899225   F-statistic:                 2.721e+07\n",
      "Date:                Tue, Feb 25 2025   P-value (F-stat)                0.0000\n",
      "Time:                        19:14:03   Distribution:                  chi2(4)\n",
      "Cov. Estimator:                robust                                         \n",
      "                                                                              \n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.0308     0.0004     86.257     0.0000      0.0301      0.0315\n",
      "x10            0.2504     0.0035     70.814     0.0000      0.2435      0.2573\n",
      "x19            0.1844     0.0043     42.763     0.0000      0.1760      0.1929\n",
      "t1             0.3321     0.0012     280.39     0.0000      0.3298      0.3344\n",
      "==============================================================================\n",
      "\n",
      "Endogenous: t1\n",
      "Instruments: z1\n",
      "Robust Covariance (Heteroskedastic)\n",
      "Debiased: False\n"
     ]
    }
   ],
   "source": [
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "# Define variables\n",
    "y = \"y1\"  # Replace with your dependent variable (e.g., test scores)\n",
    "treatment = \"t1\"  # Replace with your treatment variable\n",
    "Z = [\"z1\"]  # Replace with valid instruments\n",
    "# X = ['x1', 'x2', 'x3', 'x5', 'x6', 'x7', 'x8', 'x10', 'x11',\n",
    "    #    'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x19', 'x20', 'x21',\n",
    "    #    'x22', 'x23', 'x24', 'x25', 'x26', 'x27']  # Replace with exogenous controls\n",
    "X = ['x1', 'x10', 'x19']\n",
    "# Ensure no missing values\n",
    "# df = df.dropna()\n",
    "# print(df[Z + X].corr())\n",
    "\n",
    "# # Define 2SLS model\n",
    "model = IV2SLS(dependent=df[y], exog=df[X], endog=df[treatment], instruments=df[Z])\n",
    "\n",
    "# # Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# # Print summary\n",
    "print(results.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b40bdbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670fc3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Version: 11.8\n",
      "GPU Name: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae47b60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run -1-th experiment for NN2SLS. \n",
      "<class 'torch.Tensor'>\n",
      "Epoch 0 ended: train - 554.5031, valid - 549.3373.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 5 ended: train - 3.8782, valid - 3.8856.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 10 ended: train - 3.0731, valid - 3.0676.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 15 ended: train - 2.6925, valid - 2.7056.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 20 ended: train - 2.4826, valid - 2.5081.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 25 ended: train - 2.3603, valid - 2.3918.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 30 ended: train - 2.2420, valid - 2.2802.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 35 ended: train - 2.1887, valid - 2.2408.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 40 ended: train - 2.1281, valid - 2.1893.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 45 ended: train - 2.1169, valid - 2.1862.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 50 ended: train - 2.0954, valid - 2.1758.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 55 ended: train - 2.0180, valid - 2.1039.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 60 ended: train - 1.9158, valid - 1.9962.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 65 ended: train - 2.0656, valid - 2.1188.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 70 ended: train - 2.2706, valid - 2.3135.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 75 ended: train - 2.0603, valid - 2.1116.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 80 ended: train - 2.0728, valid - 2.1273.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 85 ended: train - 1.9764, valid - 2.0331.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 90 ended: train - 1.8780, valid - 1.9385.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 95 ended: train - 1.8218, valid - 1.8853.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 99 ended: train - 1.8321, valid - 1.8968.\n",
      "Epoch 0 ended:\n",
      "Train: loss_y: 37726.5508. \n",
      "Valid: loss_y: 37080.6836. \n",
      "Test : loss_y: 35346.7812. \n",
      "Epoch 5 ended:\n",
      "Train: loss_y: 4803.9414. \n",
      "Valid: loss_y: 4742.7598. \n",
      "Test : loss_y: 4996.5229. \n",
      "Epoch 10 ended:\n",
      "Train: loss_y: 4946.0532. \n",
      "Valid: loss_y: 5038.3071. \n",
      "Test : loss_y: 5809.8208. \n",
      "Epoch 15 ended:\n",
      "Train: loss_y: 4373.2334. \n",
      "Valid: loss_y: 4484.8706. \n",
      "Test : loss_y: 5123.2983. \n",
      "Epoch 20 ended:\n",
      "Train: loss_y: 4072.9514. \n",
      "Valid: loss_y: 4172.2349. \n",
      "Test : loss_y: 4894.4385. \n",
      "Epoch 25 ended:\n",
      "Train: loss_y: 2659.1675. \n",
      "Valid: loss_y: 2695.6353. \n",
      "Test : loss_y: 3239.1785. \n",
      "Epoch 30 ended:\n",
      "Train: loss_y: 2525.4304. \n",
      "Valid: loss_y: 2502.2734. \n",
      "Test : loss_y: 2867.7808. \n",
      "Epoch 35 ended:\n",
      "Train: loss_y: 2120.8271. \n",
      "Valid: loss_y: 2131.2283. \n",
      "Test : loss_y: 2467.4211. \n",
      "Epoch 40 ended:\n",
      "Train: loss_y: 3153.0847. \n",
      "Valid: loss_y: 3068.6926. \n",
      "Test : loss_y: 2910.0591. \n",
      "Epoch 45 ended:\n",
      "Train: loss_y: 1509.2245. \n",
      "Valid: loss_y: 1482.6898. \n",
      "Test : loss_y: 1514.9425. \n",
      "Epoch 50 ended:\n",
      "Train: loss_y: 1512.0217. \n",
      "Valid: loss_y: 1503.3000. \n",
      "Test : loss_y: 1762.3582. \n",
      "Epoch 55 ended:\n",
      "Train: loss_y: 2031.5383. \n",
      "Valid: loss_y: 1985.2028. \n",
      "Test : loss_y: 1831.5822. \n",
      "Epoch 60 ended:\n",
      "Train: loss_y: 1269.9166. \n",
      "Valid: loss_y: 1275.5299. \n",
      "Test : loss_y: 1523.2303. \n",
      "Epoch 65 ended:\n",
      "Train: loss_y: 810.9781. \n",
      "Valid: loss_y: 781.2574. \n",
      "Test : loss_y: 906.1187. \n",
      "Epoch 70 ended:\n",
      "Train: loss_y: 683.2212. \n",
      "Valid: loss_y: 691.5189. \n",
      "Test : loss_y: 892.0023. \n",
      "Epoch 75 ended:\n",
      "Train: loss_y: 642.1065. \n",
      "Valid: loss_y: 627.0318. \n",
      "Test : loss_y: 779.3895. \n",
      "Epoch 80 ended:\n",
      "Train: loss_y: 309.1777. \n",
      "Valid: loss_y: 298.7536. \n",
      "Test : loss_y: 424.9704. \n",
      "Epoch 85 ended:\n",
      "Train: loss_y: 222.4984. \n",
      "Valid: loss_y: 213.9831. \n",
      "Test : loss_y: 282.6614. \n",
      "Epoch 90 ended:\n",
      "Train: loss_y: 259.3409. \n",
      "Valid: loss_y: 251.6641. \n",
      "Test : loss_y: 320.1098. \n",
      "Epoch 95 ended:\n",
      "Train: loss_y: 328.4659. \n",
      "Valid: loss_y: 326.1920. \n",
      "Test : loss_y: 415.5809. \n",
      "Epoch 99 ended:\n",
      "Train: loss_y: 335.3555. \n",
      "Valid: loss_y: 333.4915. \n",
      "Test : loss_y: 405.2661. \n",
      "End. --------------------\n"
     ]
    }
   ],
   "source": [
    "from mliv.inference import NN2SLS\n",
    "\n",
    "data = CausalDataset('./Data/Demand/0.5_1.0_0.0_10000/1/')\n",
    "\n",
    "model = NN2SLS()\n",
    "model.fit(data)\n",
    "ITE = model.predict(data.train)\n",
    "ATE,_ = model.ATE(data.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "407d34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mliv.utils.loaddata import CausalDataset\n",
    "from mliv.inference import NN2SLS\n",
    "\n",
    "# Load data using CausalDataset (must have train.csv, valid.csv, test.csv)\n",
    "data = CausalDataset('C:\\\\Users\\\\jxiong3\\\\Documents\\\\ecma-final-project\\\\test_folder\\\\')\n",
    "\n",
    "# Move data to CPU or CUDA (if available)\n",
    "data.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "data.tensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4048dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run -1-th experiment for NN2SLS. \n",
      "<class 'torch.Tensor'>\n",
      "Epoch 0 ended: train - 9.8133, valid - 9.8157.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 5 ended: train - 9.7377, valid - 9.7320.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 10 ended: train - 9.7563, valid - 9.7491.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 15 ended: train - 9.6978, valid - 9.6926.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 20 ended: train - 9.7203, valid - 9.7156.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 25 ended: train - 9.7151, valid - 9.7110.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 30 ended: train - 9.7047, valid - 9.7003.\n",
      "<class 'torch.Tensor'>\n",
      "Epoch 35 ended: train - 9.6974, valid - 9.6928.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and Train NN2SLS\n",
    "model = NN2SLS()\n",
    "model.fit(data)\n",
    "\n",
    "# Predict Individual Treatment Effect (ITE)\n",
    "ITE = model.predict(data.train)\n",
    "\n",
    "# Estimate the Average Treatment Effect (ATE)\n",
    "ATE, _ = model.ATE(data.train)\n",
    "\n",
    "print(f\"Estimated ATE: {ATE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d6ce7f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmliv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CausalDataset\n\u001b[0;32m      3\u001b[0m train_causal \u001b[38;5;241m=\u001b[39m CausalDataset(\n\u001b[0;32m      4\u001b[0m     y\u001b[38;5;241m=\u001b[39mtrain_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      5\u001b[0m     t\u001b[38;5;241m=\u001b[39mtrain_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      6\u001b[0m     z\u001b[38;5;241m=\u001b[39mtrain_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m----> 7\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[43mcat\u001b[49m([train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx1\u001b[39m\u001b[38;5;124m\"\u001b[39m], train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx10\u001b[39m\u001b[38;5;124m\"\u001b[39m], train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx19\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m val_causal \u001b[38;5;241m=\u001b[39m CausalDataset(\n\u001b[0;32m     11\u001b[0m     y\u001b[38;5;241m=\u001b[39mval_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     12\u001b[0m     t\u001b[38;5;241m=\u001b[39mval_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     13\u001b[0m     z\u001b[38;5;241m=\u001b[39mval_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     14\u001b[0m     x\u001b[38;5;241m=\u001b[39mcat([val_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx1\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx10\u001b[39m\u001b[38;5;124m\"\u001b[39m], val_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx19\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m test_causal \u001b[38;5;241m=\u001b[39m CausalDataset(\n\u001b[0;32m     18\u001b[0m     y\u001b[38;5;241m=\u001b[39mtest_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     19\u001b[0m     t\u001b[38;5;241m=\u001b[39mtest_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     20\u001b[0m     z\u001b[38;5;241m=\u001b[39mtest_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     21\u001b[0m     x\u001b[38;5;241m=\u001b[39mcat([test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx1\u001b[39m\u001b[38;5;124m\"\u001b[39m], test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx10\u001b[39m\u001b[38;5;124m\"\u001b[39m], test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx19\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[0;32m     22\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cat' is not defined"
     ]
    }
   ],
   "source": [
    "from mliv.utils import CausalDataset\n",
    "\n",
    "train_causal = CausalDataset(\n",
    "    y=train_data[\"y1\"].reshape(-1, 1),\n",
    "    t=train_data[\"t1\"].reshape(-1, 1),\n",
    "    z=train_data[\"z1\"].reshape(-1, 1),\n",
    "    x=cat([train_data[\"x1\"], train_data[\"x10\"], train_data[\"x19\"]])\n",
    ")\n",
    "\n",
    "val_causal = CausalDataset(\n",
    "    y=val_data[\"y1\"].reshape(-1, 1),\n",
    "    t=val_data[\"t1\"].reshape(-1, 1),\n",
    "    z=val_data[\"z1\"].reshape(-1, 1),\n",
    "    x=cat([val_data[\"x1\"], val_data[\"x10\"], val_data[\"x19\"]])\n",
    ")\n",
    "\n",
    "test_causal = CausalDataset(\n",
    "    y=test_data[\"y1\"].reshape(-1, 1),\n",
    "    t=test_data[\"t1\"].reshape(-1, 1),\n",
    "    z=test_data[\"z1\"].reshape(-1, 1),\n",
    "    x=cat([test_data[\"x1\"], test_data[\"x10\"], test_data[\"x19\"]])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "352f355e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-12.550736)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d3f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run -1-th experiment for Vanilla2SLS. \n",
      "End. --------------------\n",
      "-12.550736\n",
      "Run -1-th experiment for Poly2SLS. \n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m ATE,_ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mATE(data\u001b[38;5;241m.\u001b[39mtrain)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(ATE)\n",
      "File \u001b[1;32mc:\\Users\\jxiong3\\Documents\\ecma-final-project\\mliv\\inference\\twosls\\poly2sls_v1.py:45\u001b[0m, in \u001b[0;36mPoly2SLS.fit\u001b[1;34m(self, data, exp, config)\u001b[0m\n\u001b[0;32m     43\u001b[0m pipe2 \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly\u001b[39m\u001b[38;5;124m'\u001b[39m, PolynomialFeatures()), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mridge\u001b[39m\u001b[38;5;124m'\u001b[39m, Ridge())])\n\u001b[0;32m     44\u001b[0m stage_2 \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe2, param_grid\u001b[38;5;241m=\u001b[39mparams, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m stage_2\u001b[38;5;241m.\u001b[39mfit(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, data\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_1 \u001b[38;5;241m=\u001b[39m stage_1\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "from mliv.inference import Poly2SLS\n",
    "from mliv.inference import NN2SLS\n",
    "from mliv.inference import OneSIV\n",
    "from mliv.inference import KernelIV\n",
    "from mliv.inference import DualIV\n",
    "from mliv.inference import DFL\n",
    "from mliv.inference import AGMM\n",
    "from mliv.inference import DeepGMM\n",
    "from mliv.inference import DFIV\n",
    "# from mliv.inference import DeepIV           # Tensorflow & keras\n",
    "\n",
    "for mod in [Vanilla2SLS,Poly2SLS,NN2SLS]:\n",
    "    model = mod()\n",
    "    model.config['num'] = 100\n",
    "    model.config['epochs'] = 10\n",
    "    model.fit(data)\n",
    "    ATE,_ = model.ATE(data.train)\n",
    "    print(ATE)\n",
    "#     print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ebfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path: ./Data/Demand/0.5_1.0_0.0_10000/\n",
      "Generate Demand datasets - 0/10. \n",
      "Generate Demand datasets - 1/10. \n",
      "Generate Demand datasets - 2/10. \n",
      "Generate Demand datasets - 3/10. \n",
      "Generate Demand datasets - 4/10. \n",
      "Generate Demand datasets - 5/10. \n",
      "Generate Demand datasets - 6/10. \n",
      "Generate Demand datasets - 7/10. \n",
      "Generate Demand datasets - 8/10. \n",
      "Generate Demand datasets - 9/10. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataName': 'Demand',\n",
       " 'exps': 10,\n",
       " 'num': 10000,\n",
       " 'rho': 0.5,\n",
       " 'alpha': 1.0,\n",
       " 'beta': 0.0,\n",
       " 'seed': 2022,\n",
       " 'num_val': 10000,\n",
       " 'seed_val': 3033,\n",
       " 'seed_tst': 4044}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mliv.dataset.demand import gen_data\n",
    "from mliv.utils import CausalDataset\n",
    "gen_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
